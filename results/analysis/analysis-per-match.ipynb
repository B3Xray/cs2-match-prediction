{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dccd4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/j2d2570s00g6cb7z3b2xt0zr0000gn/T/ipykernel_11504/2775948712.py:198: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(stages, rotation=45)\n",
      "/var/folders/6t/j2d2570s00g6cb7z3b2xt0zr0000gn/T/ipykernel_11504/2775948712.py:198: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(stages, rotation=45)\n",
      "/var/folders/6t/j2d2570s00g6cb7z3b2xt0zr0000gn/T/ipykernel_11504/2775948712.py:198: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(stages, rotation=45)\n",
      "/var/folders/6t/j2d2570s00g6cb7z3b2xt0zr0000gn/T/ipykernel_11504/2775948712.py:198: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(stages, rotation=45)\n",
      "/var/folders/6t/j2d2570s00g6cb7z3b2xt0zr0000gn/T/ipykernel_11504/2775948712.py:198: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(stages, rotation=45)\n",
      "/var/folders/6t/j2d2570s00g6cb7z3b2xt0zr0000gn/T/ipykernel_11504/2775948712.py:198: UserWarning: set_ticklabels() should only be used with a fixed number of ticks, i.e. after set_ticks() or using a FixedLocator.\n",
      "  ax.set_xticklabels(stages, rotation=45)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the results directory\n",
    "try:\n",
    "    # When running as a script\n",
    "    RESULTS_DIR = Path(__file__).parent.parent\n",
    "    ANALYSIS_DIR = Path(__file__).parent\n",
    "except NameError:\n",
    "    # When running in Jupyter notebook\n",
    "    RESULTS_DIR = Path.cwd().parent\n",
    "    ANALYSIS_DIR = Path.cwd()\n",
    "\n",
    "def load_matches_from_folder(folder_path: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Load all matches from a matches-cached folder.\"\"\"\n",
    "    matches = []\n",
    "    matches_path = Path(folder_path) / 'matches-cached'\n",
    "    if not matches_path.exists():\n",
    "        return matches\n",
    "        \n",
    "    for filename in matches_path.iterdir():\n",
    "        if filename.suffix == '.json':\n",
    "            with open(filename) as f:\n",
    "                match_data = json.load(f)\n",
    "                matches.append((\n",
    "                    match_data['winningTeam'],\n",
    "                    match_data['losingTeam'],\n",
    "                    filename.name\n",
    "                ))\n",
    "    return matches\n",
    "\n",
    "def analyze_model_predictions(real_world_path: str, model_path: str, stage: str) -> Dict:\n",
    "    \"\"\"Compare model predictions with real world results for a given stage.\"\"\"\n",
    "    real_matches = load_matches_from_folder(real_world_path / stage)\n",
    "    model_matches = load_matches_from_folder(model_path / stage)\n",
    "    \n",
    "    if not real_matches or not model_matches:\n",
    "        return {\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'accuracy': 0,\n",
    "            'correct_matches': [],\n",
    "            'wrong_matches': []\n",
    "        }\n",
    "\n",
    "    correct = 0\n",
    "    correct_matches = []\n",
    "    wrong_matches = []\n",
    "    analyzed_matches = 0\n",
    "    \n",
    "    # Create dictionaries for both filename and team combination lookups\n",
    "    real_results_by_file = {filename: (winner, loser) for winner, loser, filename in real_matches}\n",
    "    real_results_by_teams = {}\n",
    "    for winner, loser, filename in real_matches:\n",
    "        match_key = f\"{winner}-{loser}\"\n",
    "        alt_key = f\"{loser}-{winner}\"\n",
    "        real_results_by_teams[match_key] = (winner, loser, filename)\n",
    "        real_results_by_teams[alt_key] = (winner, loser, filename)\n",
    "    \n",
    "    # Compare predictions\n",
    "    for model_winner, model_loser, model_filename in model_matches:\n",
    "        match_key = f\"{model_winner}-{model_loser}\"\n",
    "        alt_key = f\"{model_loser}-{model_winner}\"\n",
    "        \n",
    "        # Check if this match combination actually happened (regardless of filename)\n",
    "        if match_key in real_results_by_teams or alt_key in real_results_by_teams:\n",
    "            analyzed_matches += 1\n",
    "            real_match = real_results_by_teams.get(match_key) or real_results_by_teams.get(alt_key)\n",
    "            real_winner = real_match[0]  # First element is the winner\n",
    "            \n",
    "            if real_winner == model_winner:\n",
    "                correct += 1\n",
    "                correct_matches.append(f\"{model_winner} vs {model_loser}\")\n",
    "            else:\n",
    "                wrong_matches.append(f\"{model_winner} vs {model_loser} (actual: {real_winner})\")\n",
    "\n",
    "    accuracy = (correct / analyzed_matches * 100) if analyzed_matches > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'total': analyzed_matches,\n",
    "        'accuracy': accuracy,\n",
    "        'correct_matches': correct_matches,\n",
    "        'wrong_matches': wrong_matches\n",
    "    }\n",
    "\n",
    "def generate_analysis():\n",
    "    stages = ['stage1', 'stage2', 'stage3', 'playoffs']\n",
    "    models = [\n",
    "        d for d in RESULTS_DIR.iterdir() \n",
    "        if d.is_dir() \n",
    "        and d.name != '0real-world'\n",
    "        and d.name != 'stats'\n",
    "        and d.name != 'analysis'\n",
    "        and not d.name.startswith('.')\n",
    "    ]\n",
    "    \n",
    "    # Store results for plotting\n",
    "    plot_data = {model.name: [] for model in models}\n",
    "    \n",
    "    # Generate analysis markdown file\n",
    "    analysis_file = ANALYSIS_DIR / 'analysis-per-match.md'\n",
    "    with open(analysis_file, 'w') as f:\n",
    "        f.write(\"# CS2 Match Prediction Analysis\\n\\n\")\n",
    "        \n",
    "        for model in models:\n",
    "            f.write(f\"## {model.name}\\n\\n\")\n",
    "            \n",
    "            for stage in stages:\n",
    "                results = analyze_model_predictions(\n",
    "                    RESULTS_DIR / '0real-world',\n",
    "                    model,\n",
    "                    stage\n",
    "                )\n",
    "                plot_data[model.name].append(results['accuracy'])\n",
    "                \n",
    "                f.write(f\"### {stage}\\n\")\n",
    "                f.write(f\"Accuracy: {results['accuracy']:.2f}% ({results['correct']}/{results['total']} matches)\\n\\n\")\n",
    "                \n",
    "                if results['correct_matches']:\n",
    "                    f.write(\"Correct Predictions:\\n\")\n",
    "                    for match in results['correct_matches']:\n",
    "                        f.write(f\"- {match}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                if results['wrong_matches']:\n",
    "                    f.write(\"Wrong Predictions:\\n\")\n",
    "                    for match in results['wrong_matches']:\n",
    "                        f.write(f\"- {match}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "    \n",
    "    # Styling constants\n",
    "    colors = ['#2ecc71', '#e74c3c', '#3498db', '#f1c40f', '#9b59b6', '#1abc9c']\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # 1. Line Chart\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for i, (model_name, accuracies) in enumerate(plot_data.items()):\n",
    "        plt.plot(stages, accuracies, \n",
    "                marker='o',\n",
    "                linewidth=3,\n",
    "                markersize=10,\n",
    "                label=model_name,\n",
    "                color=colors[i % len(colors)])\n",
    "\n",
    "    plt.title('Model Prediction Accuracy Across Stages', pad=20, fontsize=14)\n",
    "    plt.xlabel('Stage', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / 'analysis-per-match-line.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Faceted Line Charts\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, (model_name, accuracies) in enumerate(plot_data.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(stages, accuracies,\n",
    "                marker='o',\n",
    "                linewidth=3,\n",
    "                markersize=10,\n",
    "                color=colors[i % len(colors)])\n",
    "        ax.set_title(model_name, fontsize=12)\n",
    "        ax.set_ylim(0, 100)\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax.set_xticklabels(stages, rotation=45)\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "\n",
    "    # Hide empty subplots if any\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle('Individual Model Performance Across Stages', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / 'analysis-per-match-faceted.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    accuracy_matrix = np.array([accuracies for accuracies in plot_data.values()])\n",
    "    \n",
    "    sns.heatmap(accuracy_matrix,\n",
    "                xticklabels=stages,\n",
    "                yticklabels=plot_data.keys(),\n",
    "                annot=True,\n",
    "                fmt='.1f',\n",
    "                cmap='RdYlGn',\n",
    "                center=50,\n",
    "                vmin=0,\n",
    "                vmax=100,\n",
    "                cbar_kws={'label': 'Accuracy (%)'})\n",
    "\n",
    "    plt.title('Accuracy Heatmap Across Stages and Models', pad=20, fontsize=14)\n",
    "    plt.xlabel('Stage', fontsize=12)\n",
    "    plt.ylabel('Model', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / 'analysis-per-match-heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
