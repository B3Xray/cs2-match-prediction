{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7dccd4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from typing import Dict, List, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the results directory\n",
    "try:\n",
    "    # When running as a script\n",
    "    RESULTS_DIR = Path(__file__).parent.parent\n",
    "    ANALYSIS_DIR = Path(__file__).parent\n",
    "except NameError:\n",
    "    # When running in Jupyter notebook\n",
    "    RESULTS_DIR = Path.cwd().parent\n",
    "    ANALYSIS_DIR = Path.cwd()\n",
    "\n",
    "def load_matches_from_folder(folder_path: str) -> List[Tuple[str, str, str]]:\n",
    "    \"\"\"Load all matches from a matches-cached folder.\"\"\"\n",
    "    matches = []\n",
    "    matches_path = Path(folder_path) / 'matches-cached'\n",
    "    if not matches_path.exists():\n",
    "        return matches\n",
    "        \n",
    "    for filename in matches_path.iterdir():\n",
    "        if filename.suffix == '.json':\n",
    "            with open(filename) as f:\n",
    "                match_data = json.load(f)\n",
    "                matches.append((\n",
    "                    match_data['winningTeam'],\n",
    "                    match_data['losingTeam'],\n",
    "                    filename.name\n",
    "                ))\n",
    "    return matches\n",
    "\n",
    "def analyze_model_predictions(real_world_path: str, model_path: str, stage: str) -> Dict:\n",
    "    \"\"\"Compare model predictions with real world results for a given stage.\"\"\"\n",
    "    real_matches = load_matches_from_folder(real_world_path / stage)\n",
    "    model_matches = load_matches_from_folder(model_path / stage)\n",
    "    \n",
    "    if not real_matches or not model_matches:\n",
    "        return {\n",
    "            'correct': 0,\n",
    "            'total': 0,\n",
    "            'accuracy': 0,\n",
    "            'correct_matches': [],\n",
    "            'wrong_matches': []\n",
    "        }\n",
    "\n",
    "    correct = 0\n",
    "    correct_matches = []\n",
    "    wrong_matches = []\n",
    "    analyzed_matches = 0\n",
    "    \n",
    "    # Create dictionaries for both filename and team combination lookups\n",
    "    real_results_by_file = {filename: (winner, loser) for winner, loser, filename in real_matches}\n",
    "    real_results_by_teams = {}\n",
    "    for winner, loser, filename in real_matches:\n",
    "        match_key = f\"{winner}-{loser}\"\n",
    "        alt_key = f\"{loser}-{winner}\"\n",
    "        real_results_by_teams[match_key] = (winner, loser, filename)\n",
    "        real_results_by_teams[alt_key] = (winner, loser, filename)\n",
    "    \n",
    "    # Compare predictions\n",
    "    for model_winner, model_loser, model_filename in model_matches:\n",
    "        match_key = f\"{model_winner}-{model_loser}\"\n",
    "        alt_key = f\"{model_loser}-{model_winner}\"\n",
    "        \n",
    "        # Check if this match combination actually happened (regardless of filename)\n",
    "        if match_key in real_results_by_teams or alt_key in real_results_by_teams:\n",
    "            analyzed_matches += 1\n",
    "            real_match = real_results_by_teams.get(match_key) or real_results_by_teams.get(alt_key)\n",
    "            real_winner = real_match[0]  # First element is the winner\n",
    "            \n",
    "            if real_winner == model_winner:\n",
    "                correct += 1\n",
    "                correct_matches.append(f\"{model_winner} vs {model_loser}\")\n",
    "            else:\n",
    "                wrong_matches.append(f\"{model_winner} vs {model_loser} (actual: {real_winner})\")\n",
    "\n",
    "    accuracy = (correct / analyzed_matches * 100) if analyzed_matches > 0 else 0\n",
    "\n",
    "    return {\n",
    "        'correct': correct,\n",
    "        'total': analyzed_matches,\n",
    "        'accuracy': accuracy,\n",
    "        'correct_matches': correct_matches,\n",
    "        'wrong_matches': wrong_matches\n",
    "    }\n",
    "\n",
    "def generate_analysis():\n",
    "    stages = ['stage1', 'stage2', 'stage3', 'playoffs']\n",
    "    models = [\n",
    "        d for d in RESULTS_DIR.iterdir() \n",
    "        if d.is_dir() \n",
    "        and d.name != '0real-world'\n",
    "        and d.name != 'stats'\n",
    "        and d.name != 'analysis'\n",
    "        and not d.name.startswith('.')\n",
    "    ]\n",
    "    \n",
    "    # Store results for plotting\n",
    "    plot_data = {model.name: [] for model in models}\n",
    "    \n",
    "    # Generate analysis markdown file\n",
    "    analysis_file = ANALYSIS_DIR / 'analysis-per-match.md'\n",
    "    with open(analysis_file, 'w') as f:\n",
    "        f.write(\"# CS2 Match Prediction Analysis\\n\\n\")\n",
    "        \n",
    "        for model in models:\n",
    "            f.write(f\"## {model.name}\\n\\n\")\n",
    "            \n",
    "            for stage in stages:\n",
    "                results = analyze_model_predictions(\n",
    "                    RESULTS_DIR / '0real-world',\n",
    "                    model,\n",
    "                    stage\n",
    "                )\n",
    "                plot_data[model.name].append(results['accuracy'])\n",
    "                \n",
    "                f.write(f\"### {stage}\\n\")\n",
    "                f.write(f\"Accuracy: {results['accuracy']:.2f}% ({results['correct']}/{results['total']} matches)\\n\\n\")\n",
    "                \n",
    "                if results['correct_matches']:\n",
    "                    f.write(\"Correct Predictions:\\n\")\n",
    "                    for match in results['correct_matches']:\n",
    "                        f.write(f\"- {match}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "                \n",
    "                if results['wrong_matches']:\n",
    "                    f.write(\"Wrong Predictions:\\n\")\n",
    "                    for match in results['wrong_matches']:\n",
    "                        f.write(f\"- {match}\\n\")\n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"---\\n\\n\")\n",
    "    \n",
    "    # Styling constants\n",
    "    colors = ['#2ecc71', '#e74c3c', '#3498db', '#f1c40f', '#9b59b6', '#1abc9c']\n",
    "    plt.style.use('default')\n",
    "\n",
    "    # 1. Line Chart\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for i, (model_name, accuracies) in enumerate(plot_data.items()):\n",
    "        line = plt.plot(stages, accuracies, \n",
    "                marker='o',\n",
    "                linewidth=3,\n",
    "                markersize=10,\n",
    "                label=model_name,\n",
    "                color=colors[i % len(colors)])\n",
    "        \n",
    "        # Add accuracy numbers above each point\n",
    "        for x, y in enumerate(accuracies):\n",
    "            # Adjust vertical position if accuracy is high\n",
    "            y_offset = -15 if y > 90 else 10  # Put annotation below point if accuracy > 90%\n",
    "            \n",
    "            plt.annotate(f'{y:.1f}%', \n",
    "                    (x, y),\n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0, y_offset),\n",
    "                    ha='center',\n",
    "                    va='bottom' if y_offset > 0 else 'top',\n",
    "                    fontsize=9)\n",
    "\n",
    "    plt.title('Model Prediction Accuracy Across Stages - Analysis per matches that happened in the championship', pad=20, fontsize=14)\n",
    "    plt.xlabel('Stage', fontsize=12)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=12)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.xticks(range(len(stages)), stages, rotation=45)\n",
    "\n",
    "    # Adjust y-axis limit for annotations\n",
    "    max_accuracy = max(max(accuracies) for accuracies in plot_data.values())\n",
    "    plt.ylim(0, max(100, max_accuracy + 15))\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / 'analysis-per-match-line.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Faceted Line Charts\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i, (model_name, accuracies) in enumerate(plot_data.items()):\n",
    "        ax = axes[i]\n",
    "        ax.plot(stages, accuracies,\n",
    "                marker='o',\n",
    "                linewidth=3,\n",
    "                markersize=10,\n",
    "                color=colors[i % len(colors)])\n",
    "        \n",
    "        # Set y-axis limit with some padding for annotations\n",
    "        max_accuracy = max(accuracies)\n",
    "        y_limit = max(100, max_accuracy + 15)  # Add padding for annotations\n",
    "        ax.set_ylim(0, y_limit)\n",
    "        \n",
    "        # Add accuracy numbers above each point\n",
    "        for x, y in enumerate(accuracies):\n",
    "            # Adjust vertical position if accuracy is high\n",
    "            y_offset = -15 if y > 90 else 10  # Put annotation below point if accuracy > 90%\n",
    "            \n",
    "            ax.annotate(f'{y:.1f}%', \n",
    "                    (x, y),\n",
    "                    textcoords=\"offset points\", \n",
    "                    xytext=(0, y_offset),\n",
    "                    ha='center',\n",
    "                    va='bottom' if y_offset > 0 else 'top',\n",
    "                    fontsize=9)\n",
    "        \n",
    "        ax.set_title(model_name, fontsize=12, pad=20)  # Added padding to title\n",
    "        ax.grid(True, linestyle='--', alpha=0.7)\n",
    "        ax.set_xticks(range(len(stages)))\n",
    "        ax.set_xticklabels(stages, rotation=45)\n",
    "        ax.set_ylabel('Accuracy (%)')\n",
    "\n",
    "    # Hide empty subplots if any\n",
    "    for j in range(i + 1, len(axes)):\n",
    "        axes[j].set_visible(False)\n",
    "\n",
    "    plt.suptitle('Individual Model Performance Across Stages - Analysis per matches that happened in the championship', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / 'analysis-per-match-faceted.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 4. Heatmap\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    accuracy_matrix = np.array([accuracies for accuracies in plot_data.values()])\n",
    "\n",
    "    sns.heatmap(accuracy_matrix,\n",
    "                xticklabels=stages,\n",
    "                yticklabels=plot_data.keys(),\n",
    "                annot=True,\n",
    "                fmt='.1f',  # Format to 1 decimal place\n",
    "                annot_kws={'size': 9},\n",
    "                cmap='RdYlGn',\n",
    "                center=50,\n",
    "                vmin=0,\n",
    "                vmax=100,\n",
    "                cbar_kws={'label': 'Accuracy (%)'}\n",
    "                )\n",
    "\n",
    "    # Add % symbol to annotations after they're created\n",
    "    for t in plt.gca().texts:\n",
    "        t.set_text(t.get_text() + '%')\n",
    "\n",
    "    plt.title('Accuracy Heatmap Across Stages and Models - Analysis per matches that happened in the championship', pad=20, fontsize=14)\n",
    "    plt.xlabel('Stage', fontsize=12)\n",
    "    plt.ylabel('Model', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ANALYSIS_DIR / 'analysis-per-match-heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
