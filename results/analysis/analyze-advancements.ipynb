{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05eec644",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 156\u001b[39m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# Run the analysis\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m156\u001b[39m advancement_results = \u001b[43manalyze_advancement_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36manalyze_advancement_predictions\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34manalyze_advancement_predictions\u001b[39m():\n\u001b[32m     47\u001b[39m     \u001b[38;5;66;03m# Setup paths\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     current_dir = \u001b[43mPath\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current_dir.name == \u001b[33m'\u001b[39m\u001b[33manalysis\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     50\u001b[39m         results_dir = current_dir.parent\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_abc.py:746\u001b[39m, in \u001b[36mPathBase.cwd\u001b[39m\u001b[34m(cls)\u001b[39m\n\u001b[32m    741\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a new path pointing to the current working directory.\"\"\"\u001b[39;00m\n\u001b[32m    742\u001b[39m \u001b[38;5;66;03m# We call 'absolute()' rather than using 'os.getcwd()' directly to\u001b[39;00m\n\u001b[32m    743\u001b[39m \u001b[38;5;66;03m# enable users to replace the implementation of 'absolute()' in a\u001b[39;00m\n\u001b[32m    744\u001b[39m \u001b[38;5;66;03m# subclass and benefit from the new behaviour here. This works because\u001b[39;00m\n\u001b[32m    745\u001b[39m \u001b[38;5;66;03m# os.path.abspath('.') == os.getcwd().\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m746\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mabsolute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.3_1/Frameworks/Python.framework/Versions/3.13/lib/python3.13/pathlib/_local.py:649\u001b[39m, in \u001b[36mPath.absolute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    647\u001b[39m     cwd = os.path.abspath(\u001b[38;5;28mself\u001b[39m.drive)\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m649\u001b[39m     cwd = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetcwd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tail:\n\u001b[32m    651\u001b[39m     \u001b[38;5;66;03m# Fast path for \"empty\" paths, e.g. Path(\".\"), Path(\"\") or Path().\u001b[39;00m\n\u001b[32m    652\u001b[39m     \u001b[38;5;66;03m# We pass only one argument to with_segments() to avoid the cost\u001b[39;00m\n\u001b[32m    653\u001b[39m     \u001b[38;5;66;03m# of joining, and we exploit the fact that getcwd() returns a\u001b[39;00m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# fully-normalized string by storing it in _str. This is used to\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;66;03m# implement Path.cwd().\u001b[39;00m\n\u001b[32m    656\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._from_parsed_string(cwd)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Set\n",
    "\n",
    "def parse_markdown_standings(content: str) -> Dict[str, Dict[str, List]]:\n",
    "    \"\"\"Parse markdown content and extract standings for each stage.\"\"\"\n",
    "    stages = {}\n",
    "    current_stage = None\n",
    "    \n",
    "    # Find stage sections and their tables\n",
    "    for line in content.split('\\n'):\n",
    "        if line.startswith('### '):\n",
    "            current_stage = line.replace('### ', '').strip()\n",
    "            stages[current_stage] = []\n",
    "        elif current_stage and '|' in line and not line.startswith('|---'):\n",
    "            if 'Team | W-L |' not in line:  # Skip header row\n",
    "                parts = [p.strip() for p in line.split('|')]\n",
    "                if len(parts) >= 3:\n",
    "                    team = parts[1].strip()\n",
    "                    wl = parts[2].strip()\n",
    "                    if wl.startswith('+'):\n",
    "                        wl = wl[1:]  # Remove the '+' prefix\n",
    "                    wins, losses = map(int, wl.split('-'))\n",
    "                    stages[current_stage].append({\n",
    "                        'team': team,\n",
    "                        'wins': wins,\n",
    "                        'losses': losses,\n",
    "                        'score': wins - losses\n",
    "                    })\n",
    "    \n",
    "    # Sort teams in each stage by wins-losses\n",
    "    for stage in stages:\n",
    "        stages[stage] = sorted(\n",
    "            stages[stage],\n",
    "            key=lambda x: (x['wins'], -x['losses']),\n",
    "            reverse=True\n",
    "        )\n",
    "    \n",
    "    return stages\n",
    "\n",
    "def get_advancing_teams(stage_data: List[dict], num_teams: int = 8) -> Set[str]:\n",
    "    \"\"\"Get the top N teams from a stage based on W-L record\"\"\"\n",
    "    sorted_teams = sorted(stage_data, key=lambda x: (x['wins'], -x['losses']), reverse=True)\n",
    "    return {team['team'] for team in sorted_teams[:num_teams]}\n",
    "\n",
    "def analyze_advancement_predictions():\n",
    "    # Setup paths\n",
    "    current_dir = Path.cwd()\n",
    "    if current_dir.name == 'analysis':\n",
    "        results_dir = current_dir.parent\n",
    "    else:\n",
    "        results_dir = current_dir\n",
    "    analysis_dir = results_dir / 'analysis'\n",
    "    analysis_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Read real world results\n",
    "    with open(results_dir / '0real-world' / 'README.md') as f:\n",
    "        real_world_content = f.read()\n",
    "    real_world_standings = parse_markdown_standings(real_world_content)\n",
    "    \n",
    "    # Get real world advancing teams for each stage\n",
    "    real_advancing = {\n",
    "        'stage1': get_advancing_teams(real_world_standings['stage1']),\n",
    "        'stage2': get_advancing_teams(real_world_standings['stage2']),\n",
    "        'stage3': get_advancing_teams(real_world_standings['stage3'])\n",
    "    }\n",
    "    \n",
    "    # Get real world champion\n",
    "    playoffs_data = real_world_standings['playoffs']\n",
    "    real_champion = next(team['team'] for team in playoffs_data if team['wins'] == 3)\n",
    "    \n",
    "    # Analyze each model\n",
    "    results = {}\n",
    "    for model_dir in results_dir.iterdir():\n",
    "        if model_dir.is_dir() and model_dir.name != '0real-world' and not model_dir.name.startswith('.'):\n",
    "            model_path = model_dir / 'README.md'\n",
    "            if model_path.exists():\n",
    "                print(f\"Analyzing {model_dir.name}...\")\n",
    "                \n",
    "                # Read model predictions\n",
    "                with open(model_path) as f:\n",
    "                    model_content = f.read()\n",
    "                model_standings = parse_markdown_standings(model_content)\n",
    "                \n",
    "                # Analyze each stage\n",
    "                model_results = {\n",
    "                    'stage1': {'correct': 0, 'teams': set()},\n",
    "                    'stage2': {'correct': 0, 'teams': set()},\n",
    "                    'stage3': {'correct': 0, 'teams': set()},\n",
    "                    'champion': {'correct': False, 'predicted': ''}\n",
    "                }\n",
    "                \n",
    "                # Check advancing teams predictions\n",
    "                for stage in ['stage1', 'stage2', 'stage3']:\n",
    "                    predicted_teams = get_advancing_teams(model_standings[stage])\n",
    "                    correct_teams = predicted_teams & real_advancing[stage]\n",
    "                    model_results[stage]['correct'] = len(correct_teams)\n",
    "                    model_results[stage]['teams'] = predicted_teams\n",
    "                \n",
    "                # Check champion prediction\n",
    "                playoffs_data = model_standings['playoffs']\n",
    "                predicted_champion = next(team['team'] for team in playoffs_data if team['wins'] == 3)\n",
    "                model_results['champion']['correct'] = (predicted_champion == real_champion)\n",
    "                model_results['champion']['predicted'] = predicted_champion\n",
    "                \n",
    "                results[model_dir.name] = model_results\n",
    "    \n",
    "    # Generate report\n",
    "    report = [\"# CS2 Match Prediction Advancement Analysis\\n\\n\"]\n",
    "    \n",
    "    # Summary table\n",
    "    report.append(\"## Summary\\n\\n\")\n",
    "    report.append(\"| Model | Stage 1 | Stage 2 | Stage 3 | Champion Prediction |\\n\")\n",
    "    report.append(\"|-------|----------|----------|----------|--------------------|\")\n",
    "    \n",
    "    for model, data in results.items():\n",
    "        stage1_pct = (data['stage1']['correct'] / 8) * 100\n",
    "        stage2_pct = (data['stage2']['correct'] / 8) * 100\n",
    "        stage3_pct = (data['stage3']['correct'] / 8) * 100\n",
    "        champion = \"✓\" if data['champion']['correct'] else f\"✗ ({data['champion']['predicted']})\"\n",
    "        \n",
    "        report.append(f\"| {model} | {stage1_pct:.1f}% | {stage2_pct:.1f}% | {stage3_pct:.1f}% | {champion} |\")\n",
    "    \n",
    "    report.append(\"\\n\\n## Detailed Analysis\\n\")\n",
    "    \n",
    "    # Detailed analysis for each model\n",
    "    for model, data in results.items():\n",
    "        report.append(f\"\\n### {model}\\n\")\n",
    "        \n",
    "        for stage in ['stage1', 'stage2', 'stage3']:\n",
    "            correct = data[stage]['correct']\n",
    "            predicted = data[stage]['teams']\n",
    "            actual = real_advancing[stage]\n",
    "            \n",
    "            report.append(f\"\\n#### {stage}\")\n",
    "            report.append(f\"- Correctly predicted {correct}/8 teams ({(correct/8)*100:.1f}%)\")\n",
    "            report.append(\"- Missed predictions:\")\n",
    "            report.append(\"  - Should have advanced: \" + \", \".join(sorted(actual - predicted)))\n",
    "            report.append(\"  - Should not have advanced: \" + \", \".join(sorted(predicted - actual)))\n",
    "        \n",
    "        report.append(f\"\\n#### Playoffs\")\n",
    "        report.append(f\"- Predicted champion: {data['champion']['predicted']}\")\n",
    "        report.append(f\"- Actual champion: {real_champion}\")\n",
    "        report.append(f\"- Prediction {'correct' if data['champion']['correct'] else 'incorrect'}\")\n",
    "        \n",
    "        report.append(\"\\n---\\n\")\n",
    "    \n",
    "    # Save report\n",
    "    with open(analysis_dir / 'advancement-analysis.md', 'w') as f:\n",
    "        f.write('\\n'.join(report))\n",
    "    \n",
    "    print(f\"Analysis complete. Results saved in {analysis_dir}/analyze-advancements.md\")\n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "advancement_results = analyze_advancement_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56458b1",
   "metadata": {},
   "source": [
    "# Data Viz\n",
    "\n",
    "which charts can we show based on the above analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a1895d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "def create_visualizations(results: Dict, analysis_dir: Path):\n",
    "    # Convert results to DataFrame for easier plotting\n",
    "    stage_data = []\n",
    "    for model, data in results.items():\n",
    "        for stage in ['stage1', 'stage2', 'stage3']:\n",
    "            stage_data.append({\n",
    "                'Model': model,\n",
    "                'Stage': stage.replace('stage', 'Stage '),\n",
    "                'Accuracy': (data[stage]['correct'] / 8) * 100\n",
    "            })\n",
    "    df = pd.DataFrame(stage_data)\n",
    "\n",
    "    # 1. Heatmap with original color scaling\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    accuracy_matrix = df.pivot(index='Model', columns='Stage', values='Accuracy')\n",
    "    \n",
    "    sns.heatmap(accuracy_matrix, \n",
    "                annot=True, \n",
    "                fmt='.1f',\n",
    "                cmap='RdYlGn',\n",
    "                center=50)\n",
    "    \n",
    "    # Add % to annotations\n",
    "    for t in plt.gca().texts:\n",
    "        t.set_text(t.get_text() + '%')\n",
    "    \n",
    "    plt.title('Team Advancement Prediction Accuracy by Model and Stage', pad=20, size=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(analysis_dir / 'accuracy_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Bar plot\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    overall_accuracy = df.groupby('Model')['Accuracy'].mean().sort_values(ascending=False)\n",
    "    ax = overall_accuracy.plot(kind='bar', color=sns.color_palette('viridis', len(overall_accuracy)))\n",
    "    plt.title('Average Team Advancement Prediction Accuracy Across All Stages', pad=20, size=14)\n",
    "    plt.ylabel('Average Accuracy (%)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add value labels on top of bars\n",
    "    for i, v in enumerate(overall_accuracy):\n",
    "        ax.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(analysis_dir / 'overall_performance.png', dpi=300, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def add_visualization_section(report: List[str]):\n",
    "    report.append(\"\\n## Visualizations\\n\\n\")\n",
    "    report.append(\"### Accuracy Heatmap\\n\")\n",
    "    report.append(\"The heatmap below shows the accuracy percentage for each model across different stages. \"+\n",
    "                 \"Green indicates higher accuracy, while red indicates lower accuracy.\\n\\n\")\n",
    "    report.append(\"![Accuracy Heatmap](accuracy_heatmap.png)\\n\\n\")\n",
    "    report.append(\"### Overall Model Performance\\n\")\n",
    "    report.append(\"Average prediction accuracy across all stages for each model:\\n\\n\")\n",
    "    report.append(\"![Overall Performance](overall_performance.png)\\n\\n\")\n",
    "current_dir = Path.cwd()\n",
    "if current_dir.name == 'analysis':\n",
    "    results_dir = current_dir.parent\n",
    "else:\n",
    "    results_dir = current_dir\n",
    "analysis_dir = results_dir / 'analysis'\n",
    "analysis_dir.mkdir(exist_ok=True)\n",
    "create_visualizations(advancement_results, analysis_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
